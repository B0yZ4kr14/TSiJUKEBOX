# ğŸ Guia de Testes Python do Instalador TSiJUKEBOX

Este guia cobre as prÃ¡ticas de teste, ferramentas e convenÃ§Ãµes para os testes Python do instalador TSiJUKEBOX.

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [ConfiguraÃ§Ã£o do Ambiente](#configuraÃ§Ã£o-do-ambiente)
- [Estrutura de Testes](#estrutura-de-testes)
- [Executando Testes](#executando-testes)
- [Escrevendo Novos Testes](#escrevendo-novos-testes)
- [Cobertura de CÃ³digo](#cobertura-de-cÃ³digo)
- [Testes de SeguranÃ§a](#testes-de-seguranÃ§a)
- [Testes de Performance](#testes-de-performance)
- [CI/CD](#cicd)
- [Debugging](#debugging)
- [Contribuindo](#contribuindo)
- [Recursos](#recursos)

---

## VisÃ£o Geral

O instalador TSiJUKEBOX usa uma estratÃ©gia de testes em mÃºltiplas camadas para garantir qualidade e confiabilidade:

| Camada | Ferramenta | LocalizaÃ§Ã£o | PropÃ³sito |
|--------|------------|-------------|-----------|
| UnitÃ¡rio | pytest | `scripts/tests/test_*.py` | LÃ³gica de componentes isolados |
| IntegraÃ§Ã£o | pytest | `scripts/tests/test_*_integration.py` | Fluxos entre componentes |
| Edge Cases | pytest | `scripts/tests/test_*_edge_cases.py` | CenÃ¡rios extremos |
| SeguranÃ§a | pytest | `scripts/tests/test_*_security.py` | ValidaÃ§Ã£o de seguranÃ§a |
| Benchmark | pytest-benchmark | `scripts/tests/test_*_benchmark.py` | Performance |
| E2E | pytest + Docker | `scripts/tests/e2e/` | InstalaÃ§Ã£o completa em containers |

### MÃ©tricas Atuais

- **30+** arquivos de teste
- **500+** testes individuais
- **60%** cobertura mÃ­nima exigida
- **7** jobs no CI/CD

---

## ConfiguraÃ§Ã£o do Ambiente

### Requisitos

- Python 3.10+
- pip ou pipenv
- Docker (para testes E2E)

### InstalaÃ§Ã£o

```bash
# Navegar para o diretÃ³rio de scripts
cd scripts

# Criar ambiente virtual (recomendado)
python -m venv venv
source venv/bin/activate  # Linux/macOS
# ou
.\venv\Scripts\activate   # Windows

# Instalar dependÃªncias de teste
pip install -r requirements-test.txt
```

### DependÃªncias de Teste

O arquivo `requirements-test.txt` inclui:

```txt
pytest>=8.0.0
pytest-cov>=4.1.0
pytest-mock>=3.12.0
pytest-asyncio>=0.23.0
pytest-timeout>=2.2.0
pytest-benchmark>=4.0.0
coverage[toml]>=7.4.0
python-on-whales>=0.70.0
```

---

## Estrutura de Testes

```
scripts/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py                        # Fixtures compartilhadas
â”‚   â”œâ”€â”€ pytest.ini                         # ConfiguraÃ§Ã£o pytest
â”‚   â”‚
â”‚   â”œâ”€â”€ # Testes UnitÃ¡rios
â”‚   â”œâ”€â”€ test_unified_installer.py          # Instalador principal
â”‚   â”œâ”€â”€ test_cli_parsing.py                # Parser de argumentos CLI
â”‚   â”œâ”€â”€ test_audio_setup.py                # ConfiguraÃ§Ã£o de Ã¡udio
â”‚   â”œâ”€â”€ test_database_setup.py             # Setup de banco de dados
â”‚   â”œâ”€â”€ test_fonts_setup.py                # InstalaÃ§Ã£o de fontes
â”‚   â”œâ”€â”€ test_ntp_setup.py                  # SincronizaÃ§Ã£o de tempo
â”‚   â”œâ”€â”€ test_ufw_setup.py                  # ConfiguraÃ§Ã£o de firewall
â”‚   â”œâ”€â”€ test_spicetify_setup.py            # Setup Spicetify
â”‚   â”‚
â”‚   â”œâ”€â”€ # Testes de Edge Cases
â”‚   â”œâ”€â”€ test_unified_installer_edge_cases.py
â”‚   â”‚
â”‚   â”œâ”€â”€ # Testes de IntegraÃ§Ã£o
â”‚   â”œâ”€â”€ test_unified_installer_integration.py
â”‚   â”‚
â”‚   â”œâ”€â”€ # Testes de SeguranÃ§a
â”‚   â”œâ”€â”€ test_rls_security.py               # PolÃ­ticas RLS do Supabase
â”‚   â”œâ”€â”€ test_installer_security.py         # SeguranÃ§a do instalador
â”‚   â”‚
â”‚   â”œâ”€â”€ # Testes de Performance
â”‚   â”œâ”€â”€ test_installer_benchmark.py
â”‚   â”‚
â”‚   â””â”€â”€ e2e/                               # Testes E2E em Docker
â”‚       â”œâ”€â”€ Dockerfile.test
â”‚       â”œâ”€â”€ docker-compose.test.yml
â”‚       â””â”€â”€ test_installer_docker_e2e.py
â”‚
â”œâ”€â”€ run-coverage.sh                        # Script helper para cobertura
â””â”€â”€ requirements-test.txt                  # DependÃªncias de teste
```

---

## Executando Testes

### Comandos BÃ¡sicos

```bash
cd scripts

# Executar todos os testes (exceto docker/e2e)
pytest tests/ -v

# Testes com cobertura completa (recomendado)
./run-coverage.sh

# Apenas testes unitÃ¡rios
./run-coverage.sh unit

# Testes rÃ¡pidos (<2s cada)
./run-coverage.sh quick

# Testes de integraÃ§Ã£o
./run-coverage.sh integration

# Testes de edge cases
./run-coverage.sh edge
```

### Executando Testes EspecÃ­ficos

```bash
# Arquivo especÃ­fico
pytest tests/test_audio_setup.py -v

# Classe especÃ­fica
pytest tests/test_audio_setup.py::TestAudioSetup -v

# Teste especÃ­fico
pytest tests/test_audio_setup.py::TestAudioSetup::test_install_pipewire -v

# Por marker
pytest -m "not slow" -v      # Excluir testes lentos
pytest -m "security" -v      # Apenas testes de seguranÃ§a
pytest -m "unit" -v          # Apenas unitÃ¡rios
```

### Markers DisponÃ­veis

| Marker | DescriÃ§Ã£o |
|--------|-----------|
| `unit` | Testes unitÃ¡rios |
| `integration` | Testes de integraÃ§Ã£o |
| `e2e` | Testes end-to-end |
| `docker` | Requer Docker |
| `benchmark` | Testes de performance |
| `security` | Testes de seguranÃ§a |
| `slow` | Testes lentos (>5s) |
| `network` | Requer acesso Ã  rede |

---

## Escrevendo Novos Testes

### ConvenÃ§Ãµes de Nomenclatura

| Elemento | PadrÃ£o | Exemplo |
|----------|--------|---------|
| Arquivo | `test_<modulo>.py` | `test_audio_setup.py` |
| Classe | `Test<Feature>` | `TestAudioSetup` |
| MÃ©todo | `test_<comportamento>` | `test_install_pipewire` |
| Fixture | `<recurso>` | `mock_subprocess` |

### Estrutura de Teste PadrÃ£o

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Testes para o mÃ³dulo XYZ.

Run with: pytest tests/test_xyz.py -v
"""

import pytest
from unittest.mock import MagicMock, patch


class TestXYZFeature:
    """Testa funcionalidade XYZ."""

    @pytest.fixture
    def setup_xyz(self, mocker):
        """Fixture para configurar XYZ."""
        mocker.patch('subprocess.run')
        return XYZ()

    def test_behavior_when_condition(self, setup_xyz):
        """Descreve o comportamento esperado quando condiÃ§Ã£o."""
        # Arrange
        expected = "resultado esperado"
        
        # Act
        result = setup_xyz.do_something()
        
        # Assert
        assert result == expected

    def test_raises_error_when_invalid_input(self, setup_xyz):
        """Deve levantar erro com entrada invÃ¡lida."""
        with pytest.raises(ValueError, match="mensagem esperada"):
            setup_xyz.process(invalid_input)
```

### Exemplo Completo

```python
import pytest
from unittest.mock import MagicMock, patch, call

class TestAudioSetup:
    """Testa configuraÃ§Ã£o de Ã¡udio do instalador."""

    @pytest.fixture
    def audio_setup(self, mocker):
        """Cria instÃ¢ncia de AudioSetup com mocks."""
        mocker.patch('subprocess.run')
        mocker.patch('shutil.which', return_value='/usr/bin/pipewire')
        
        from installer.audio_setup import AudioSetup
        return AudioSetup(backend='pipewire')

    @pytest.fixture
    def mock_run(self, mocker):
        """Mock para subprocess.run."""
        return mocker.patch(
            'subprocess.run',
            return_value=MagicMock(returncode=0, stdout='', stderr='')
        )

    def test_install_pipewire_success(self, audio_setup, mock_run):
        """PipeWire deve ser instalado com sucesso."""
        # Act
        result = audio_setup.install()

        # Assert
        assert result is True
        mock_run.assert_called()
        
        # Verificar pacotes instalados
        calls = [str(c) for c in mock_run.call_args_list]
        assert any('pipewire' in c for c in calls)

    def test_install_fails_gracefully(self, audio_setup, mock_run):
        """Falha na instalaÃ§Ã£o deve ser tratada corretamente."""
        mock_run.return_value = MagicMock(returncode=1, stderr='Error')

        result = audio_setup.install()

        assert result is False

    @pytest.mark.parametrize("backend", ["pipewire", "pulseaudio", "alsa"])
    def test_supports_multiple_backends(self, backend, mocker):
        """Deve suportar mÃºltiplos backends de Ã¡udio."""
        mocker.patch('subprocess.run')
        
        from installer.audio_setup import AudioSetup
        setup = AudioSetup(backend=backend)
        
        assert setup.backend == backend
```

### Usando Fixtures do conftest.py

```python
# conftest.py fornece fixtures compartilhadas

def test_with_temp_directory(temp_install_dir):
    """Usa diretÃ³rio temporÃ¡rio para instalaÃ§Ã£o."""
    config_file = temp_install_dir / "config.ini"
    config_file.write_text("[main]\nenabled=true")
    
    assert config_file.exists()

def test_with_mock_distro(mock_distro_info):
    """Usa informaÃ§Ãµes de distro mockadas."""
    assert mock_distro_info['name'] == 'Ubuntu'
    assert mock_distro_info['version'] == '22.04'
```

---

## Cobertura de CÃ³digo

### ConfiguraÃ§Ã£o

A cobertura Ã© configurada em `pytest.ini`:

```ini
[coverage:run]
source = .
omit = 
    tests/*
    */__pycache__/*
branch = True

[coverage:report]
show_missing = True
precision = 2
fail_under = 60
```

### Gerando RelatÃ³rios

```bash
# RelatÃ³rio no terminal
pytest --cov=. --cov-report=term-missing tests/

# RelatÃ³rio HTML
pytest --cov=. --cov-report=html tests/
open htmlcov/index.html

# RelatÃ³rio XML (para CI)
pytest --cov=. --cov-report=xml tests/
```

### Metas de Cobertura

| MÃ³dulo | Meta | DescriÃ§Ã£o |
|--------|------|-----------|
| `installer/` | 70% | CÃ³digo principal do instalador |
| `utils/` | 80% | UtilitÃ¡rios |
| `cli/` | 60% | Interface de linha de comando |
| **Total** | **60%** | **MÃ­nimo exigido** |

---

## Testes de SeguranÃ§a

### Categorias de Testes

1. **PolÃ­ticas RLS** (`test_rls_security.py`)
   - ValidaÃ§Ã£o de Row-Level Security
   - ProteÃ§Ã£o de tabelas admin
   - Isolamento de dados por usuÃ¡rio
   - PrevenÃ§Ã£o de escalaÃ§Ã£o de privilÃ©gios

2. **SeguranÃ§a do Instalador** (`test_installer_security.py`)
   - PrevenÃ§Ã£o de injeÃ§Ã£o de comandos
   - Manuseio seguro de credenciais
   - PermissÃµes de arquivos
   - ConfiguraÃ§Ã£o de rede

### Executando Testes de SeguranÃ§a

```bash
# Todos os testes de seguranÃ§a
pytest tests/test_*security*.py -v

# Apenas RLS
pytest tests/test_rls_security.py -v

# Apenas seguranÃ§a do instalador
pytest tests/test_installer_security.py -v

# Com relatÃ³rio de cobertura
pytest tests/test_*security*.py --cov=. --cov-report=html -v
```

### Exemplo de Teste de SeguranÃ§a

```python
class TestCommandInjectionPrevention:
    """Testa prevenÃ§Ã£o de injeÃ§Ã£o de comandos."""

    @pytest.mark.parametrize("payload", [
        "; rm -rf /",
        "| cat /etc/passwd",
        "$(whoami)",
        "`id`",
    ])
    def test_username_sanitization(self, payload):
        """Username deve rejeitar payloads de injeÃ§Ã£o."""
        valid_pattern = re.compile(r'^[a-zA-Z0-9_-]+$')
        
        assert not valid_pattern.match(payload), \
            f"Payload de injeÃ§Ã£o nÃ£o deveria passar: {payload}"
```

---

## Testes de Performance

### Executando Benchmarks

```bash
# Todos os benchmarks
pytest tests/test_installer_benchmark.py --benchmark-only -v

# Com comparaÃ§Ã£o
pytest tests/test_installer_benchmark.py \
    --benchmark-compare \
    --benchmark-autosave

# Salvar resultados em JSON
pytest tests/test_installer_benchmark.py \
    --benchmark-json=benchmark-results.json
```

### Exemplo de Benchmark

```python
class TestInstallerBenchmark:
    """Benchmarks de performance do instalador."""

    def test_benchmark_config_parsing(self, benchmark):
        """Benchmark de parsing de configuraÃ§Ã£o."""
        config_content = "[main]\nenabled=true\n" * 100
        
        result = benchmark(parse_config, config_content)
        
        assert result is not None

    def test_benchmark_phase_dependency_check(self, benchmark):
        """Benchmark de verificaÃ§Ã£o de dependÃªncias."""
        def check_deps():
            # Simula verificaÃ§Ã£o
            return check_all_dependencies()
        
        result = benchmark(check_deps)
        assert result['missing'] == []
```

---

## CI/CD

### Workflow GitHub Actions

Os testes Python sÃ£o executados automaticamente via `.github/workflows/python-installer-tests.yml`:

```yaml
# Jobs executados:
# 1. lint - VerificaÃ§Ã£o de cÃ³digo com ruff
# 2. unit-tests - Testes unitÃ¡rios + cobertura
# 3. edge-case-tests - Testes de edge cases
# 4. integration-tests - Testes de integraÃ§Ã£o
# 5. benchmark-tests - Benchmarks (apenas main/develop)
# 6. e2e-docker-tests - E2E em Docker (apenas main)
# 7. test-status - Status final
```

### Triggers

- **Push** para `main` ou `develop` (arquivos em `scripts/`)
- **Pull Request** para `main` ou `develop` (arquivos em `scripts/`)
- **Manual** via workflow dispatch

### Artefatos Gerados

| Artefato | RetenÃ§Ã£o | DescriÃ§Ã£o |
|----------|----------|-----------|
| `python-coverage-html` | 30 dias | RelatÃ³rio HTML de cobertura |
| `python-coverage-xml` | 7 dias | RelatÃ³rio XML para anÃ¡lise |
| `installer-benchmark-results` | 30 dias | Resultados de benchmark em JSON |

---

## Debugging

### Modo Debug

```bash
# Entrar no debugger em falha
pytest --pdb tests/test_audio_setup.py

# Breakpoint especÃ­fico
pytest --pdb-trace tests/test_audio_setup.py::TestAudioSetup::test_install

# Verbose mÃ¡ximo
pytest -vvs tests/test_audio_setup.py
```

### Logging em Testes

```python
import logging

def test_with_logging(caplog):
    """Teste com captura de logs."""
    with caplog.at_level(logging.DEBUG):
        result = do_something()
    
    assert "Expected message" in caplog.text
```

### Debugging de Fixtures

```bash
# Mostrar fixtures disponÃ­veis
pytest --fixtures tests/

# Setup/teardown verbose
pytest --setup-show tests/test_audio_setup.py
```

---

## Contribuindo

### Checklist para Novos Testes

- [ ] Seguir convenÃ§Ãµes de nomenclatura
- [ ] Usar fixtures do `conftest.py` quando disponÃ­veis
- [ ] Adicionar markers apropriados (`@pytest.mark.unit`, etc.)
- [ ] Documentar com docstrings claras
- [ ] Usar padrÃ£o Arrange-Act-Assert
- [ ] Incluir testes de casos de erro
- [ ] Manter testes isolados e independentes

### Processo de ContribuiÃ§Ã£o

1. **Fork** o repositÃ³rio
2. **Crie branch**: `git checkout -b feat/meu-teste`
3. **Escreva testes** seguindo as convenÃ§Ãµes
4. **Execute localmente**: `./run-coverage.sh`
5. **Verifique cobertura**: mÃ­nimo 60%
6. **Commit**: `git commit -m "test: adiciona testes para XYZ"`
7. **Push**: `git push origin feat/meu-teste`
8. **Abra PR** com descriÃ§Ã£o clara

### Boas PrÃ¡ticas

1. **Teste comportamento, nÃ£o implementaÃ§Ã£o**
2. **Um assert por teste** (quando prÃ¡tico)
3. **Nomes descritivos**: "should [action] when [condition]"
4. **Mock dependÃªncias externas**
5. **Evite testes frÃ¡geis** que dependem de ordem ou estado global

---

## Recursos

### DocumentaÃ§Ã£o

- [pytest Documentation](https://docs.pytest.org/)
- [pytest-cov](https://pytest-cov.readthedocs.io/)
- [pytest-mock](https://pytest-mock.readthedocs.io/)
- [pytest-benchmark](https://pytest-benchmark.readthedocs.io/)

### Guias

- [Testing Best Practices](https://docs.pytest.org/en/stable/explanation/goodpractices.html)
- [Mocking in Python](https://realpython.com/python-mock-library/)

### ReferÃªncias Internas

- [docs/TESTING.md](TESTING.md) - Testes JavaScript/TypeScript
- [scripts/pytest.ini](../scripts/pytest.ini) - ConfiguraÃ§Ã£o pytest
- [scripts/run-coverage.sh](../scripts/run-coverage.sh) - Script helper

---

<p align="center">
  <em>Testes sÃ£o documentaÃ§Ã£o que nunca mente.</em>
</p>
